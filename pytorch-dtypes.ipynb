{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "def probe_device_dtypes(device):\n",
    "    \"\"\"Dynamically probe the device for supported dtypes.\"\"\"\n",
    "    all_dtypes = [\n",
    "        torch.float64, torch.float32, torch.float16, torch.bfloat16,\n",
    "        torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64,\n",
    "        torch.bool, torch.complex64, torch.complex128\n",
    "    ]\n",
    "    \n",
    "    supported_dtypes = []\n",
    "    matrix_size = 64  # Small size for quick probing\n",
    "    \n",
    "    for dtype in all_dtypes:\n",
    "        try:\n",
    "            if dtype in [torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64]:\n",
    "                matrix_a = torch.randint(0, 100, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "                matrix_b = torch.randint(0, 100, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "            elif dtype == torch.bool:\n",
    "                matrix_a = torch.randint(0, 2, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "                matrix_b = torch.randint(0, 2, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "            elif dtype in [torch.complex64, torch.complex128]:\n",
    "                matrix_a = torch.randn(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "                matrix_b = torch.randn(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "            else:\n",
    "                matrix_a = torch.rand(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "                matrix_b = torch.rand(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "            \n",
    "            _ = torch.matmul(matrix_a, matrix_b)\n",
    "            if \"cuda\" in device:\n",
    "                torch.cuda.synchronize()\n",
    "            elif \"xpu\" in device:\n",
    "                torch.xpu.synchronize()\n",
    "            \n",
    "            supported_dtypes.append(dtype)\n",
    "        except (RuntimeError, TypeError, AttributeError):\n",
    "            continue\n",
    "    \n",
    "    return supported_dtypes, [dt for dt in all_dtypes if dt not in supported_dtypes]\n",
    "\n",
    "def check_amp_support(device, dtype):\n",
    "    \"\"\"Check if AMP is supported for the device and dtype.\"\"\"\n",
    "    amp_dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "    if \"cuda\" in device and torch.cuda.is_available():\n",
    "        return dtype in amp_dtypes\n",
    "    elif \"xpu\" in device and hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "        return dtype in amp_dtypes\n",
    "    elif device == \"cpu\":\n",
    "        return dtype in amp_dtypes and hasattr(torch, \"cpu\") and hasattr(torch.cpu, \"amp\")\n",
    "    return False\n",
    "\n",
    "def check_compile_support(device):\n",
    "    \"\"\"Check if torch.compile is supported and functional.\"\"\"\n",
    "    if not hasattr(torch, \"compile\"):\n",
    "        return False\n",
    "    # Test compilation with a simple operation\n",
    "    try:\n",
    "        compiled_fn = torch.compile(lambda x, y: x + y)\n",
    "        test_tensor = torch.ones(1).to(device)\n",
    "        _ = compiled_fn(test_tensor, test_tensor)\n",
    "        if \"cuda\" in device:\n",
    "            torch.cuda.synchronize()\n",
    "        elif \"xpu\" in device:\n",
    "            torch.xpu.synchronize()\n",
    "        return True\n",
    "    except (RuntimeError, TypeError, AttributeError):\n",
    "        return False\n",
    "\n",
    "def run_benchmark(device, dtype, matrix_size=4096, epochs=10, warmup_runs=2, use_amp=False, use_compile=False):\n",
    "    \"\"\"Run benchmark for a specific dtype with AMP and torch.compile options.\"\"\"\n",
    "    if dtype in [torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64]:\n",
    "        matrix_a = torch.randint(0, 100, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "        matrix_b = torch.randint(0, 100, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "    elif dtype == torch.bool:\n",
    "        matrix_a = torch.randint(0, 2, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "        matrix_b = torch.randint(0, 2, (matrix_size, matrix_size), dtype=dtype).to(device)\n",
    "    elif dtype in [torch.complex64, torch.complex128]:\n",
    "        matrix_a = torch.randn(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "        matrix_b = torch.randn(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "    else:\n",
    "        matrix_a = torch.rand(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "        matrix_b = torch.rand(matrix_size, matrix_size, dtype=dtype).to(device)\n",
    "\n",
    "    # Compile matmul if requested and supported\n",
    "    matmul_fn = torch.compile(torch.matmul) if use_compile and check_compile_support(device) else torch.matmul\n",
    "\n",
    "    # Warmup runs\n",
    "    for _ in range(warmup_runs):\n",
    "        if use_amp and check_amp_support(device, dtype):\n",
    "            with torch.autocast(device_type=device.split(\":\")[0], dtype=torch.float16 if \"cuda\" in device else torch.bfloat16):\n",
    "                _ = matmul_fn(matrix_a, matrix_b)\n",
    "        else:\n",
    "            _ = matmul_fn(matrix_a, matrix_b)\n",
    "    if \"cuda\" in device:\n",
    "        torch.cuda.synchronize()\n",
    "    elif \"xpu\" in device:\n",
    "        torch.xpu.synchronize()\n",
    "\n",
    "    # Main benchmark\n",
    "    start_time = time.time()\n",
    "    if \"cuda\" in device:\n",
    "        torch.cuda.synchronize()\n",
    "    elif \"xpu\" in device:\n",
    "        torch.xpu.synchronize()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if use_amp and check_amp_support(device, dtype):\n",
    "            with torch.autocast(device_type=device.split(\":\")[0], dtype=torch.float16 if \"cuda\" in device else torch.bfloat16):\n",
    "                result = matmul_fn(matrix_a, matrix_b)\n",
    "        else:\n",
    "            result = matmul_fn(matrix_a, matrix_b)\n",
    "        if \"cuda\" in device:\n",
    "            torch.cuda.synchronize()\n",
    "        elif \"xpu\" in device:\n",
    "            torch.xpu.synchronize()\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Completed epoch {epoch + 1}/{epochs} for {str(dtype).split('.')[-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    if \"cuda\" in device:\n",
    "        torch.cuda.synchronize()\n",
    "    elif \"xpu\" in device:\n",
    "        torch.xpu.synchronize()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    operations = matrix_size ** 3 * epochs\n",
    "    \n",
    "    if total_time <= 0:\n",
    "        print(f\"Warning: Total time for {str(dtype).split('.')[-1]} was zero or negative, setting GOPS to 0\")\n",
    "        gops = 0.0\n",
    "    else:\n",
    "        gops = (operations / total_time) / 1e9\n",
    "    \n",
    "    return total_time, gops\n",
    "\n",
    "def run_multi_dtype_benchmark(device_str=\"cuda:0\", matrix_size=4096, epochs=10, warmup_runs=2, runs=1):\n",
    "    # Determine device\n",
    "    if \"cuda\" in device_str and torch.cuda.is_available():\n",
    "        device = device_str\n",
    "        device_name = torch.cuda.get_device_name(device)\n",
    "    elif \"xpu\" in device_str and hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "        device = device_str\n",
    "        device_name = torch.xpu.get_device_name(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        device_name = \"CPU\"\n",
    "    \n",
    "    # Check AMP and compile support\n",
    "    compile_supported = check_compile_support(device)\n",
    "    avx512_support = device == \"cpu\" and hasattr(torch.backends, \"cpu\") and hasattr(torch.backends.cpu, \"has_avx512\") and torch.backends.cpu.has_avx512()\n",
    "\n",
    "    # Display initial info\n",
    "    print(f\"Running on: {device} ({device_name})\")\n",
    "    if device == \"cpu\":\n",
    "        print(f\"AVX512 Support: {'Yes' if avx512_support else 'No'}\")\n",
    "    print(f\"Matrix size: {matrix_size}x{matrix_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Warmup runs: {warmup_runs}\")\n",
    "    print(f\"Number of runs: {runs}\")\n",
    "    print(f\"torch.compile Supported: {compile_supported}\")\n",
    "    print(f\"AMP Supported: {'Partially (float32, float16, bfloat16)' if any(check_amp_support(device, dt) for dt in [torch.float32, torch.float16, torch.bfloat16]) else 'No'}\\n\")\n",
    "\n",
    "    # Probe supported dtypes\n",
    "    print(\"Probing device for supported dtypes...\")\n",
    "    supported_dtypes, unsupported_dtypes = probe_device_dtypes(device)\n",
    "    print(f\"Supported dtypes: {[str(dt).split('.')[-1] for dt in supported_dtypes]}\\n\")\n",
    "\n",
    "    # Generate all permutations\n",
    "    amp_options = [False, True] if any(check_amp_support(device, dt) for dt in supported_dtypes) else [False]\n",
    "    compile_options = [False, True] if compile_supported else [False]\n",
    "    permutations = list(itertools.product(amp_options, compile_options))\n",
    "\n",
    "    # Store results\n",
    "    all_results = {}\n",
    "\n",
    "    for amp, comp in permutations:\n",
    "        config_name = f\"AMP:{'ON' if amp else 'OFF'}, Compile:{'ON' if comp else 'OFF'}\"\n",
    "        print(f\"\\nTesting Configuration: {config_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        config_results = {dtype: {\"times\": [], \"gops_list\": []} for dtype in supported_dtypes}\n",
    "        \n",
    "        for run in range(runs):\n",
    "            print(f\"\\nRun {run + 1}/{runs}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for dtype in supported_dtypes:\n",
    "                print(f\"Running benchmark for {str(dtype).split('.')[-1]}\")\n",
    "                total_time, gops = run_benchmark(device, dtype, matrix_size, epochs, warmup_runs, use_amp=amp, use_compile=comp)\n",
    "                config_results[dtype][\"times\"].append(total_time)\n",
    "                config_results[dtype][\"gops_list\"].append(gops)\n",
    "                \n",
    "                dtype_name = str(dtype).split('.')[-1]\n",
    "                print(f\"{dtype_name:<12} Total Time: {total_time:<6.2f}s  GOPS: {gops:<6.2f}  AMP: {amp:<5} Compile: {comp}\")\n",
    "\n",
    "        # Calculate and print averages for this configuration\n",
    "        print(f\"\\nAverage Results for {config_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Dtype':<12} {'Avg Total Time (s)':<20} {'Avg GOPS':<12} {'AMP':<8} {'Compile':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for dtype in supported_dtypes:\n",
    "            dtype_name = str(dtype).split('.')[-1]\n",
    "            avg_time = sum(config_results[dtype][\"times\"]) / runs\n",
    "            avg_gops = sum(config_results[dtype][\"gops_list\"]) / runs\n",
    "            print(f\"{dtype_name:<12} {avg_time:<20.2f} {avg_gops:<12.2f} {str(amp):<8} {str(comp):<8}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        all_results[config_name] = config_results\n",
    "\n",
    "    # Print supported and unsupported dtypes\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Supported dtypes: {[str(dt).split('.')[-1] for dt in supported_dtypes]}\")\n",
    "    print(f\"Unsupported dtypes: {[str(dt).split('.')[-1] for dt in unsupported_dtypes]}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Example usage in notebook\n",
    "matrix_size = 4096\n",
    "epochs = 100\n",
    "warmup_runs = 20\n",
    "runs = 3\n",
    "device = \"cuda:0\"  # Options: \"cuda:0\", \"xpu:0\", \"cpu\"\n",
    "\n",
    "results = run_multi_dtype_benchmark(\n",
    "    device_str=device,\n",
    "    matrix_size=matrix_size,\n",
    "    epochs=epochs,\n",
    "    warmup_runs=warmup_runs,\n",
    "    runs=runs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
